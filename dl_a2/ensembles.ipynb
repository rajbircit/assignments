{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ensembles.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajbircit/assignments/blob/main/dl_a2/ensembles.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbYaJtuDaTSE"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbaCPhN3m9aK"
      },
      "source": [
        "#### Show the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB1IYEiCVI31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d44faccc-d767-4a65-a2a6-eea5d2fbccd2"
      },
      "source": [
        "!nvidia-smi\n",
        "print()\n",
        "print()\n",
        "print('=' * 80)\n",
        "print()\n",
        "print()\n",
        "!df -h\n",
        "print()\n",
        "print()\n",
        "print('=' * 80)\n",
        "print()\n",
        "print()\n",
        "!free -m\n",
        "print()\n",
        "print()\n",
        "print('=' * 80)\n",
        "print()\n",
        "print()\n",
        "!lscpu\n",
        "print()\n",
        "print()\n",
        "print('=' * 80)\n",
        "print()\n",
        "print()\n",
        "! ps -eo pmem,pcpu,vsize,pid,cmd | sort -k 1 -nr | head -5\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Apr 24 16:11:09 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   69C    P0    32W /  70W |   5128MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Filesystem      Size  Used Avail Use% Mounted on\n",
            "overlay          69G   40G   29G  58% /\n",
            "tmpfs            64M     0   64M   0% /dev\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/fs/cgroup\n",
            "shm             5.8G     0  5.8G   0% /dev/shm\n",
            "/dev/sda1        75G   42G   33G  57% /opt/bin\n",
            "tmpfs           6.4G  100K  6.4G   1% /var/colab\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/acpi\n",
            "tmpfs           6.4G     0  6.4G   0% /proc/scsi\n",
            "tmpfs           6.4G     0  6.4G   0% /sys/firmware\n",
            "drive            69G   41G   28G  60% /content/gdrive\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "              total        used        free      shared  buff/cache   available\n",
            "Mem:          13021        7148         162          15        5710       10936\n",
            "Swap:             0           0           0\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Architecture:        x86_64\n",
            "CPU op-mode(s):      32-bit, 64-bit\n",
            "Byte Order:          Little Endian\n",
            "CPU(s):              2\n",
            "On-line CPU(s) list: 0,1\n",
            "Thread(s) per core:  2\n",
            "Core(s) per socket:  1\n",
            "Socket(s):           1\n",
            "NUMA node(s):        1\n",
            "Vendor ID:           GenuineIntel\n",
            "CPU family:          6\n",
            "Model:               79\n",
            "Model name:          Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "Stepping:            0\n",
            "CPU MHz:             2199.998\n",
            "BogoMIPS:            4399.99\n",
            "Hypervisor vendor:   KVM\n",
            "Virtualization type: full\n",
            "L1d cache:           32K\n",
            "L1i cache:           32K\n",
            "L2 cache:            256K\n",
            "L3 cache:            56320K\n",
            "NUMA node0 CPU(s):   0,1\n",
            "Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "\n",
            "\n",
            "================================================================================\n",
            "\n",
            "\n",
            "58.4 33.5 20410772  3185 /usr/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-aac5aeeb-fff9-4d1c-8257-26bb47e3ee13.json\n",
            " 0.8  0.3 2481528    225 /opt/google/drive/drive --features=fuse_max_background:1000,max_read_qps:1000,max_write_qps:1000,max_operation_batch_size:15,max_parallel_push_task_instances:10,opendir_timeout_ms:120000,virtual_folders_omit_spaces:true --inet_family=IPV4_ONLY --preferences=trusted_root_certs_file_path:/opt/google/drive/roots.pem,mount_point_path:/content/gdrive --console_auth --auto_restart_count=1 --first_auto_restart_timestamp=169 --single_process --parent_version=46.0.3.0 --crash_handler_token=dummy_crash_token\n",
            " 0.4  0.7 202636      50 /usr/bin/python2 /usr/local/bin/jupyter-notebook --ip=\"172.28.0.2\" --port=9000 --FileContentsManager.root_dir=\"/\" --LargeFileManager.delete_to_trash=False --MappingKernelManager.root_dir=\"/content\"\n",
            " 0.3  0.4 357628       1 /tools/node/bin/node /datalab/web/app.js\n",
            " 0.1  0.2 129180    3205 /usr/bin/python3 /usr/local/lib/python3.7/dist-packages/debugpy/adapter --for-server 44467 --host 127.0.0.1 --port 22525 --server-access-token 4c365e04119a2c93798e238551aa8b2a295d5a893bcbbf88e8775edd96d62997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HhKgZdLnDXe"
      },
      "source": [
        "#### Setup logging"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-qJFYSyXipf"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "import numpy as np\n",
        "import logging\n",
        "import sys\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import zipfile\n",
        "import h5py\n",
        "import gc\n",
        "import IPython\n",
        "\n",
        "np.random.seed(1)\n",
        "random.seed(1)\n",
        "tf.random.set_seed(1)\n",
        "\n",
        "root = logging.getLogger()\n",
        "root.setLevel(logging.INFO)\n",
        "\n",
        "handler = logging.StreamHandler(sys.stdout)\n",
        "handler.setLevel(logging.DEBUG)\n",
        "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
        "handler.setFormatter(formatter)\n",
        "root.addHandler(handler)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5dT_-3nTqYe"
      },
      "source": [
        "#### Mount Google Drive And Copy Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrZWOv9fTsF2",
        "outputId": "578f889f-8b24-496a-e6f7-6c841436f19a"
      },
      "source": [
        "drive.mount('/content/gdrive')\n",
        "\n",
        "if (os.path.exists(\"/root/imagedata\")):\n",
        "    ret = shutil.rmtree(\"/root/imagedata\")\n",
        "os.mkdir(\"/root/imagedata\")\n",
        "shutil.copyfile( \\\n",
        "    \"/content/gdrive/MyDrive/DeepLearningAssignment2/earth_data.zip\",\n",
        "    \"/root/imagedata/earth_data.zip\")\n",
        "\n",
        "!cd /root/imagedata && unzip earth_data.zip && rm -f earth_data.zip"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "Archive:  earth_data.zip\n",
            "  inflating: earth_data.h5           \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hK1gQ5CV1i-"
      },
      "source": [
        "## Code to plot graphs and remember histories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy4Y9ACqfea7"
      },
      "source": [
        "class Entry:\n",
        "    def __init__(self, loss, val_loss, accuracy, val_accuracy, best_accuracy, best_loss):\n",
        "        self.loss = loss\n",
        "        self.val_loss = val_loss\n",
        "        self.accuracy = accuracy\n",
        "        self.val_accuracy = val_accuracy\n",
        "        self.best_accuracy = best_accuracy\n",
        "        self.best_loss = best_loss\n",
        "\n",
        "class Plot:\n",
        "    def __init__(self):\n",
        "        self.history = {}\n",
        "        self.colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'tab:blue', \\\n",
        "                       'tab:orange', 'tab:green', 'tab:red', 'tab:purple', \\\n",
        "                       'tab:brown', 'tab:pink', 'tab:olive', 'tab:cyan']\n",
        "\n",
        "    def plot_history(self, history, N_EPOCHS, name, show=True):\n",
        "        if show:\n",
        "            N_EPOCHS = len(history.history[\"loss\"])\n",
        "            plt.rcParams[\"figure.figsize\"] = (20, 10)\n",
        "            plt.xticks(np.arange(0, N_EPOCHS+1, 1.0))\n",
        "            plt.plot(np.arange(0, N_EPOCHS), history.history[\"loss\"], label=\"train loss\", color='blue', linestyle='solid')\n",
        "            plt.plot(np.arange(0, N_EPOCHS), history.history[\"val_loss\"], label=\"val loss\", color='blue', linestyle='dashdot')\n",
        "            plt.plot(np.arange(0, N_EPOCHS), history.history[\"accuracy\"], label=\"train acc\", color='red', linestyle='solid')\n",
        "            plt.plot(np.arange(0, N_EPOCHS), history.history[\"val_accuracy\"], label=\"val acc\", color='red', linestyle='dashdot')\n",
        "            plt.legend()\n",
        "            plt.show()\n",
        "        best_val_acc = max(history.history['val_accuracy'])\n",
        "        index_best_val_accuracy = history.history['val_accuracy'].index(best_val_acc)\n",
        "        best_val_loss = min(history.history['val_loss'])\n",
        "        index_best_val_loss = history.history['val_loss'].index(best_val_loss)\n",
        "        if show:\n",
        "            print(f\"Best validation accuracy: {best_val_acc}, epoch = {index_best_val_accuracy}\")\n",
        "            print(f\"Best validation loss: {best_val_loss}, epoch = {index_best_val_loss}\")\n",
        "        entry = Entry(\\\n",
        "                      loss=history.history[\"loss\"],\\\n",
        "                      val_loss=history.history[\"val_loss\"],\\\n",
        "                      accuracy=history.history[\"accuracy\"],\\\n",
        "                      val_accuracy=history.history[\"val_accuracy\"],\\\n",
        "                      best_accuracy=best_val_acc,\n",
        "                      best_loss=best_val_loss)\n",
        "        self.history[name] = entry\n",
        "        gc.collect()\n",
        "\n",
        "    def superplot(self):\n",
        "        i = 0\n",
        "        plt.rcParams[\"figure.figsize\"] = [30, 10]\n",
        "        fig, ax = plt.subplots(1, 2)\n",
        "        def innerplot(self, ax, arr, text, linest):\n",
        "            ax.plot(np.arange(0, len(arr)), arr, label=text, color=self.colors[i], linestyle=linest)\n",
        "        for name, entry in self.history.items():\n",
        "            innerplot(self, ax[0], entry.loss, f\"{name}-loss\", \"solid\")\n",
        "            innerplot(self, ax[0], entry.val_loss, f\"{name}-val-loss\", \"dashdot\")\n",
        "            innerplot(self, ax[1], entry.accuracy, f\"{name}-accuracy\", \"solid\")\n",
        "            innerplot(self, ax[1], entry.val_accuracy, f\"{name}-val-accuracy\", \"dashdot\")\n",
        "            i += 1\n",
        "        ax[0].legend()\n",
        "        ax[1].legend()\n",
        "        plt.show()\n",
        "\n"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZF7YJVg4Z8j3"
      },
      "source": [
        "## Extract train and test instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opVq7JewaAGA",
        "outputId": "48438629-f04d-43ec-bc18-5b90112f0a6e"
      },
      "source": [
        "def loadDataH5():\n",
        "    with h5py.File('/root/imagedata/earth_data.h5','r') as hf:\n",
        "        trainX = np.array(hf.get('trainX'))\n",
        "        trainY = np.array(hf.get('trainY'))\n",
        "        valX = np.array(hf.get('valX'))\n",
        "        valY = np.array(hf.get('valY'))\n",
        "        print (trainX.shape,trainY.shape)\n",
        "        print (valX.shape,valY.shape)\n",
        "        return trainX, trainY, valX, valY\n",
        "\n",
        "trainX, trainY, valX, valY = loadDataH5()\n",
        "trainX = trainX / 255.0\n",
        "valX = valX / 255.0"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19200, 64, 64, 3) (19200,)\n",
            "(4800, 64, 64, 3) (4800,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewCcmX_SWH4W"
      },
      "source": [
        "## Callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLKhKGMRWNfT"
      },
      "source": [
        "def decay_after_runs(N):\n",
        "    NRUNS = N\n",
        "    # We're sneaking this in, since this will be called at every epoch\n",
        "    # it also gives us a good way to force calls to gc() within the fit function\n",
        "    gc.collect()\n",
        "    def learning_rate_scheduler(epoch, lr):\n",
        "        if NRUNS < 0 or epoch < NRUNS:\n",
        "            return lr\n",
        "        else:\n",
        "            print(f\"Learning Rate: {lr} --> {lr * tf.math.exp(-1.0)}\")\n",
        "            return lr * tf.math.exp(-0.1)\n",
        "    return learning_rate_scheduler\n",
        "lr_callback = tf.keras.callbacks.LearningRateScheduler(decay_after_runs(20))\n",
        "\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\\\n",
        "                                                  monitor='val_loss',\\\n",
        "                                                  patience=3,\n",
        "                                                  verbose=1,\n",
        "                                                  mode='auto')\n",
        "early_stopping2 = tf.keras.callbacks.EarlyStopping(\\\n",
        "                                                  monitor='val_loss',\\\n",
        "                                                  patience=10,\n",
        "                                                  verbose=1,\n",
        "                                                  mode='auto')\n",
        "\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\\\n",
        "                                                 monitor='val_loss',\\\n",
        "                                                 factor=0.5,\\\n",
        "                                                 patience=3,\\\n",
        "                                                 min_lr=0.001)\n",
        "\n",
        "term_on_nan = tf.keras.callbacks.TerminateOnNaN()\n",
        "\n",
        "DRIVE_FOLDER = \"/content/gdrive/MyDrive/DeepLearningAssignment2\"\n",
        "\n",
        "def get_checkpoint_callback(name):\n",
        "    checkpoint_filepath = f\"{DRIVE_FOLDER}/ckp-{name}\"\n",
        "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_filepath,\n",
        "        save_weights_only=True,\n",
        "        monitor='val_accuracy',\n",
        "        mode='max',\n",
        "        save_best_only=True)\n",
        "    return model_checkpoint_callback\n",
        "\n",
        "\n",
        "def get_callbacks_decay_after(N, name=None):\n",
        "    if N == -2:\n",
        "        callback_array = [\\\n",
        "                        tf.keras.callbacks.LearningRateScheduler(\\\n",
        "                                                    decay_after_runs(-1)),\n",
        "                        early_stopping2,\n",
        "                        term_on_nan,\n",
        "                        ]\n",
        "    else:\n",
        "        callback_array = [\\\n",
        "                        tf.keras.callbacks.LearningRateScheduler(\\\n",
        "                                                    decay_after_runs(N)),\n",
        "                        early_stopping,\n",
        "                        term_on_nan,\n",
        "                        ]\n",
        "    if N > 0:\n",
        "        callback_array.append(reduce_lr)\n",
        "    if None != name:\n",
        "        callback_array.append(get_checkpoint_callback(name))\n",
        "    return callback_array"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NWZ5Iw4skLdB"
      },
      "source": [
        "# Question 1 Part A"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVwezZ7dkTt_"
      },
      "source": [
        "## 1. Build a model generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwizNaadbIpk"
      },
      "source": [
        "import random\n",
        "import gc\n",
        "class ModelFactory:\n",
        "    @staticmethod\n",
        "    def get_conv_layer(depth, inshape=None, force_stride_size=False):\n",
        "        out_layers = []\n",
        "        filter_size = random.choice([2, 3, 4, 5])\n",
        "        stride_size = random.choice([1, 2, 3])\n",
        "        stride_size = stride_size if stride_size < filter_size else filter_size\n",
        "\n",
        "        # In case there is an exception, the caller will\n",
        "        # set this and call again\n",
        "        stride_size = 1 if force_stride_size else stride_size\n",
        "\n",
        "        stride_size = (stride_size, stride_size, )\n",
        "        filter_shape = tuple([filter_size, filter_size,])\n",
        "        act_fn = random.choice(['tanh', 'relu', 'sigmoid', 'selu', 'elu'])\n",
        "        pd = random.choice(['same', 'valid'])\n",
        "\n",
        "        if None != inshape:\n",
        "            conv_layer = tf.keras.layers.Conv2D(\\\n",
        "                                                depth,\\\n",
        "                                                filter_shape,\\\n",
        "                                                padding=pd,\\\n",
        "                                                input_shape=inshape,\\\n",
        "                                                strides=stride_size,\\\n",
        "                                                activation=act_fn)\n",
        "        else:\n",
        "            conv_layer = tf.keras.layers.Conv2D(\\\n",
        "                                                depth,\\\n",
        "                                                filter_shape,\\\n",
        "                                                padding=pd,\\\n",
        "                                                strides=stride_size,\\\n",
        "                                                activation=act_fn)\n",
        "        out_layers.append(conv_layer)\n",
        "\n",
        "        if random.choice([True, False]):\n",
        "            pool_size = random.choice([2, 3, 4])\n",
        "            out_layers.append(tf.keras.layers.MaxPooling2D(\\\n",
        "                                                           pool_size,\\\n",
        "                                                           pool_size,))\n",
        "        \n",
        "        if random.choice([True, False]):\n",
        "            out_layers.append(tf.keras.layers.SpatialDropout2D(0.25))\n",
        "\n",
        "        return out_layers\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_model_internal(inshp, nlabels:int, modelname:str, fss:bool):\n",
        "        layer_neurons = []\n",
        "        n_conv_layers = random.choice([1, 2, 3,])\n",
        "        layer_depth = sorted([\\\n",
        "                random.choice([32, 64, 128, 256, 512, 1024]) \\\n",
        "                    for _ in range(n_conv_layers)])\n",
        "\n",
        "        for i in range(1, len(layer_depth)):\n",
        "            if layer_depth[i] <= layer_depth[i-1]:\n",
        "                layer_depth[i] = 2 * layer_depth[i-1]\n",
        "\n",
        "        # Add the first Convolutional layer\n",
        "        model = tf.keras.Sequential(name=modelname)\n",
        "        for lyr in ModelFactory.get_conv_layer(layer_depth[0], inshp, force_stride_size=fss):\n",
        "            model.add(lyr)\n",
        "        \n",
        "        # Add zero or more additional convoluationa layer\n",
        "        for d in layer_depth[1:]:\n",
        "            for lyr in ModelFactory.get_conv_layer(d, inshape=None, force_stride_size=fss):\n",
        "                model.add(lyr)\n",
        "\n",
        "        # Flatten\n",
        "        model.add(tf.keras.layers.Flatten())\n",
        "\n",
        "        # Add zero or more dense layers\n",
        "        n_dense = random.choice([0, 1, 2])\n",
        "        for _ in range(n_dense - 1):\n",
        "            act_fn = random.choice(['tanh', 'relu', 'sigmoid', 'selu', 'elu'])\n",
        "            layer = tf.keras.layers.Dense(\\\n",
        "                                random.choice([32, 64, 128, 256, 512]),\n",
        "                                activation=act_fn)\n",
        "            model.add(layer)\n",
        "            if random.choice([True, False]):\n",
        "                layer = tf.keras.layers.Dropout(0.25)\n",
        "                model.add(layer)\n",
        "        \n",
        "        # Add the final Dense layer\n",
        "        model.add(tf.keras.layers.Dense(nlabels, activation='softmax'))\n",
        "        \n",
        "        return model\n",
        "\n",
        "    @staticmethod\n",
        "    def print_model(model):\n",
        "        plot = tf.keras.utils.plot_model(model, show_shapes=True, expand_nested=True)\n",
        "        IPython.display.display(plot)\n",
        "        print(model.summary())\n",
        "\n",
        "    @staticmethod\n",
        "    def generate_model_size(inshp:tuple, nlabels:int, modelname:str, max_params:int):\n",
        "        if (-1 == max_params):\n",
        "            try:\n",
        "                model = ModelFactory.generate_model_internal(\\\n",
        "                                                            inshp,\\\n",
        "                                                            nlabels,\\\n",
        "                                                            modelname,\\\n",
        "                                                            False)\n",
        "            except:\n",
        "                model = ModelFactory.generate_model_internal(\\\n",
        "                                                            inshp,\\\n",
        "                                                            nlabels,\\\n",
        "                                                            modelname,\\\n",
        "                                                            True)\n",
        "            ModelFactory.print_model(model)\n",
        "            return model\n",
        "        else:\n",
        "            # Make 10 attempts to generate a model with the number of\n",
        "            # trainable params less than the one specified\n",
        "            for i in range(10):\n",
        "                gc.collect()\n",
        "                logging.debug(f\"Attempt {i}\")\n",
        "                try:\n",
        "                    model = ModelFactory.generate_model_internal(\\\n",
        "                                                                inshp,\\\n",
        "                                                                nlabels,\\\n",
        "                                                                modelname,\n",
        "                                                                False)\n",
        "                except:\n",
        "                    model = ModelFactory.generate_model_internal(\\\n",
        "                                                                inshp,\\\n",
        "                                                                nlabels,\\\n",
        "                                                                modelname,\n",
        "                                                                True)\n",
        "                if model.count_params() < max_params:\n",
        "                    ModelFactory.print_model(model)\n",
        "                    gc.collect()\n",
        "                    return model\n",
        "\n",
        "                gc.collect()\n",
        "            return None\n",
        "    \n",
        "    @staticmethod\n",
        "    def generate_model(inshp:tuple, nlabels:int, modelname:str, max_params:int):\n",
        "        \"\"\"\n",
        "        Tries to make a model with the number of trainable params less\n",
        "        than the specified number. If it is not able to do so, it will\n",
        "        create a model with a greater number regardless\n",
        "        \"\"\"\n",
        "        model = ModelFactory.generate_model_size(\\\n",
        "                                                 inshp,\\\n",
        "                                                 nlabels,\\\n",
        "                                                 modelname,\\\n",
        "                                                 max_params)\n",
        "        if None == model:\n",
        "            model = ModelFactory.generate_model_size(\\\n",
        "                                                     inshp,\\\n",
        "                                                     nlabels,\\\n",
        "                                                     modelname,\\\n",
        "                                                     -1)\n",
        "        return model"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNCqleXXKLq-"
      },
      "source": [
        "# Generate the Base Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTDI6AqQKDz0"
      },
      "source": [
        "\n",
        "def get_formatted_json(jsonstr):\n",
        "    j = json.loads(jsonstr)\n",
        "    return json.dumps(j, indent=4)\n",
        "\n",
        "get_model_name = lambda n : f\"ensemble-base-model-{n}\"\n",
        "get_model_json_filename = lambda n : f\"save-ensemble-base-model-{n}.json\"\n",
        "\n",
        "import json\n",
        "def generate_base_models(n_models):\n",
        "    n_classes = len(np.unique(np.concatenate((trainY, valY))))\n",
        "    for i in range(n_models):\n",
        "        modelname = get_model_name(i)\n",
        "        filename = get_model_json_filename(i)\n",
        "        model = ModelFactory.generate_model(trainX[0].shape, n_classes, modelname, 9_500_000)\n",
        "        model_json = model.to_json()\n",
        "        with open(f\"{DRIVE_FOLDER}/{filename}\", \"w\") as f:\n",
        "            f.write(get_formatted_json(model_json))\n",
        "            f.close()"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qICfAnC2OOZg"
      },
      "source": [
        "# Train each model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU4ACEwVOMcO"
      },
      "source": [
        "def train_model_internal(model, plot):\n",
        "    print(model.summary())\n",
        "    print()\n",
        "    N_EPOCHS = 2\n",
        "    model.compile(optimizer='adam', \\\n",
        "              loss='sparse_categorical_crossentropy', \\\n",
        "              metrics=['accuracy'])\n",
        "    if model.count_params() < 1_500_000:\n",
        "        print(\"Using decay of learning reate\")\n",
        "        cb = get_callbacks_decay_after(5, model.name)\n",
        "    else:\n",
        "        print(\"Avoiding decay of learning reate\")\n",
        "        cb = get_callbacks_decay_after(-1, model.name)\n",
        "    history = model.fit(trainX, trainY, \\\n",
        "                    validation_data=(valX, valY),\\\n",
        "                    batch_size=32,\\\n",
        "                    epochs=N_EPOCHS,\n",
        "                    callbacks=cb)\n",
        "    if plot:\n",
        "        plot.plot_history(history, 100, model.name)\n",
        "\n",
        "def train_model(i, plot):\n",
        "    print('=' * 80)\n",
        "    print('=' * 80)\n",
        "    print(f\"Training model {i}\")\n",
        "    print()\n",
        "    fname = get_model_json_filename(i)\n",
        "    fname = f\"{DRIVE_FOLDER}/{fname}\"\n",
        "    with open(fname, \"r\") as f:\n",
        "        jsonstr = f.read()\n",
        "        model = tf.keras.models.model_from_json(jsonstr)\n",
        "        train_model_internal(model, plot)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23ULZugvX-Fi"
      },
      "source": [
        "def load_model(n):\n",
        "    fname = get_model_json_filename(n)\n",
        "    fname = f\"{DRIVE_FOLDER}/{fname}\"\n",
        "    with open(fname, \"r\") as f:\n",
        "        jsonstr = f.read()\n",
        "        model = tf.keras.models.model_from_json(jsonstr)\n",
        "        model.compile(optimizer='adam', \\\n",
        "              loss='sparse_categorical_crossentropy', \\\n",
        "              metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "def load_model_weight(model, n):\n",
        "    weight_file = f\"{DRIVE_FOLDER}/ckp-{model.name}\"\n",
        "    model.load_weights(weight_file)\n",
        "\n",
        "def get_predictions(model, X):\n",
        "    x = model.predict(X)\n",
        "    return x\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1i6gkAITGsAl"
      },
      "source": [
        "N_BASE_MODELS = 10\n",
        "\n",
        "train_pred_fname = lambda i: f\"{DRIVE_FOLDER}/train_pred_{i}.txt\"\n",
        "val_pred_fname = lambda i: f\"{DRIVE_FOLDER}/val_pred_{i}.txt\"\n",
        "\n",
        "def train_all_models(plot):\n",
        "    for i in range(N_BASE_MODELS):\n",
        "        train_model(i, plot)\n",
        "\n",
        "def evaluate_and_save(i):\n",
        "    print(f\"--------------------- {i} -----------------------\")\n",
        "    model = load_model(i)\n",
        "    load_model_weight(model, i)\n",
        "    trainPred = get_predictions(model, trainX)\n",
        "    valPred = get_predictions(model, valX)\n",
        "    np.savetxt(train_pred_fname(i), trainPred, delimiter=',')\n",
        "    np.savetxt(val_pred_fname(i), valPred, delimiter=',')\n",
        "    gc.collect()\n",
        "\n",
        "def evaluate_and_save_all():\n",
        "    for i in range(N_BASE_MODELS):\n",
        "        evaluate_and_save(i)\n",
        "\n",
        "def get_train_predictions(N):\n",
        "    ret = []\n",
        "    for i in range(N):\n",
        "        ret.append(np.loadtxt(train_pred_fname(i), delimiter=','))\n",
        "    return ret\n",
        "\n",
        "def get_val_predictions(N):\n",
        "    ret = []\n",
        "    for i in range(N):\n",
        "        ret.append(np.loadtxt(val_pred_fname(i), delimiter=','))\n",
        "    return ret\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "def get_accuracy(y_true, y_pred):\n",
        "    y_pred = np.argmax(y_pred, axis=1)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    return acc\n",
        "\n",
        "def get_ensemble_predictions(pred_array):\n",
        "    added = pred_array[0].copy()\n",
        "    for a in pred_array[1:]:\n",
        "        added = added + a\n",
        "    return added\n",
        "\n",
        "import seaborn as sns\n",
        "def simple_ensemble():\n",
        "    val_accuracies = []\n",
        "    train_accuracies = []\n",
        "    model_names = []\n",
        "\n",
        "    # Load the saved training predictions\n",
        "    train_predictions = get_train_predictions(N_BASE_MODELS)\n",
        "\n",
        "    # Get the ensemble prediction for the training\n",
        "    #    by summing over the predictions for each base learner\n",
        "    ensemble_train_prediction = get_ensemble_predictions(train_predictions)\n",
        "\n",
        "    # Load the saved validation predictions\n",
        "    val_predictions = get_val_predictions(N_BASE_MODELS)\n",
        "\n",
        "    # Get the ensemble prediction for the validation\n",
        "    #    by summing over the predictions for each base learner\n",
        "    ensemble_val_prediction = get_ensemble_predictions(val_predictions)\n",
        "\n",
        "    # Get the accuracy for the validation predictions for each base models\n",
        "    for n, i in enumerate(val_predictions):\n",
        "        acc = get_accuracy(valY, i)\n",
        "        val_accuracies.append(acc)\n",
        "\n",
        "    # Append the validation accuracy of the ensemble\n",
        "    val_accuracies.append(get_accuracy(valY, ensemble_val_prediction))\n",
        "\n",
        "    # Get the accuracy for the training predictions for each base model\n",
        "    for n, i in enumerate(train_predictions):\n",
        "        acc = get_accuracy(trainY, i)\n",
        "        train_accuracies.append(acc)\n",
        "\n",
        "    # Append the training accuracy for the ensemble\n",
        "    train_accuracies.append(get_accuracy(trainY, ensemble_train_prediction))\n",
        "\n",
        "    # Add the model names\n",
        "    for i in range(len(train_predictions)):\n",
        "        model_names.append(f\"Model - {i}\")\n",
        "    model_names.append(\"Ensemble\")\n",
        "\n",
        "    # Create a dataframe\n",
        "    val_accuracies = pd.Series(val_accuracies)\n",
        "    train_accuracies = pd.Series(train_accuracies)\n",
        "    model_names = pd.Series(model_names)\n",
        "    df = pd.DataFrame(\\\n",
        "        { \\\n",
        "            \"val_accuracies\": val_accuracies,\\\n",
        "            \"train_accuracies\": train_accuracies,\\\n",
        "            \"model_names\": model_names\\\n",
        "        }\\\n",
        "    )\n",
        "\n",
        "    plot = sns.barplot(y='model_names', x='val_accuracies', data=df, orient='h')\n",
        "    plot.set_title('Simple Ensemble (adding all predictions) (validation set)')\n",
        "    plt.show()\n",
        "\n",
        "    plot = sns.barplot(y='model_names', x='train_accuracies', data=df, orient='h')\n",
        "    plot.set_title('Simple Ensemble (adding all predictions) (training set)')\n",
        "    plt.show()\n",
        "\n",
        "\n"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4KiNlfvv-un"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "def rf_ensemble():\n",
        "    val_accuracies = []\n",
        "    train_accuracies = []\n",
        "    model_names = []\n",
        "\n",
        "    # Load the saved training predictions\n",
        "    train_predictions = get_train_predictions(N_BASE_MODELS)\n",
        "\n",
        "    # Get the ensemble prediction for the training\n",
        "    #    by summing over the predictions for each base learner\n",
        "    ensemble_train_features = get_ensemble_predictions(train_predictions)\n",
        "\n",
        "    # Load the saved validation predictions\n",
        "    val_predictions = get_val_predictions(N_BASE_MODELS)\n",
        "\n",
        "    # Get the ensemble prediction for the validation\n",
        "    #    by summing over the predictions for each base learner\n",
        "    ensemble_val_features = get_ensemble_predictions(val_predictions)\n",
        "\n",
        "    for j in train_predictions:\n",
        "        ensemble_train_features = np.append(ensemble_train_features, j, axis=1)\n",
        "    for j in val_predictions:\n",
        "        ensemble_val_features = np.append(ensemble_val_features, j, axis=1)\n",
        "\n",
        "    rfc = RandomForestClassifier()\n",
        "    rfc.fit(ensemble_train_features, trainY)\n",
        "    y_pred = rfc.predict(ensemble_val_features)\n",
        "    print(accuracy_score(valY, y_pred))\n",
        "\n"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-ne45g1v7xl"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "def main():\n",
        "\n",
        "    #plot = Plot()\n",
        "    #generate_base_models(N_BASE_MODELS)\n",
        "    #print(\"Training all models ...\")\n",
        "    #train_all_models(plot)\n",
        "    #print(\"Done\")\n",
        "    #print(\"Evaluate and save ...\")\n",
        "    #evaluate_and_save_all()\n",
        "    #print(\"Done\")\n",
        "    #simple_ensemble()\n",
        "    rf_ensemble()\n"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otFamfCBK11G",
        "outputId": "0e4a9fc2-70d2-4103-e3c8-0a265ce8e31a"
      },
      "source": [
        "main()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8358333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ORE0aHmLhkL",
        "outputId": "31507af7-d446-4578-ec72-60a6e5b22ac4"
      },
      "source": [
        "gc.collect()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFkJ9XTxvyZu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}